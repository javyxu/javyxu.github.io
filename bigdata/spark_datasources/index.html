<!DOCTYPE html>
<html lang="zh">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"javyxu.cn","root":"/","scheme":"Gemini","version":"7.8.0","exturl":false,"sidebar":{"position":"right","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"manual","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta name="description" content="Python处理各种Spark数据源Spark SQL通过DataFrame接口支持对各种数据源进行操作。DataFrame可以使用关系型数据库转换操作，也可以用来创建临时视图。将一个DataFrame注册为临时视图允许您对其数据运行SQL查询。现在本人介绍使用Spark加载和保存数据的基本使用方法，然后介绍内置数据源的读取和保存。 数据的加载和保存 普通方式加载和保存spark默认数据类型par">
<meta property="og:type" content="article">
<meta property="og:title" content="Python处理各种Spark数据源">
<meta property="og:url" content="https://javyxu.cn/bigdata/spark_datasources/index.html">
<meta property="og:site_name" content="Tech Blog">
<meta property="og:description" content="Python处理各种Spark数据源Spark SQL通过DataFrame接口支持对各种数据源进行操作。DataFrame可以使用关系型数据库转换操作，也可以用来创建临时视图。将一个DataFrame注册为临时视图允许您对其数据运行SQL查询。现在本人介绍使用Spark加载和保存数据的基本使用方法，然后介绍内置数据源的读取和保存。 数据的加载和保存 普通方式加载和保存spark默认数据类型par">
<meta property="og:locale">
<meta property="article:published_time" content="2019-01-08T13:30:00.000Z">
<meta property="article:modified_time" content="2021-02-21T02:37:33.298Z">
<meta property="article:author" content="Javy">
<meta property="article:tag" content="BigData">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="https://javyxu.cn/bigdata/spark_datasources/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh'
  };
</script>

  <title>Python处理各种Spark数据源 | Tech Blog</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Tech Blog</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">Javy's Blog</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-algorithm">

    <a href="/algorithm/" rel="section"><i class="fa fa-code fa-fw"></i>Algorithm</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>Search
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="Searching..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>

  <a href="https://github.com/javyxu" class="github-corner" title="Follow me on GitHub" aria-label="Follow me on GitHub" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh">
    <link itemprop="mainEntityOfPage" href="https://javyxu.cn/bigdata/spark_datasources/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.jpg">
      <meta itemprop="name" content="Javy">
      <meta itemprop="description" content="You can create art and beauty on a computer.">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Tech Blog">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          Python处理各种Spark数据源
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2019-01-08 21:30:00" itemprop="dateCreated datePublished" datetime="2019-01-08T21:30:00+08:00">2019-01-08</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2021-02-21 10:37:33" itemprop="dateModified" datetime="2021-02-21T10:37:33+08:00">2021-02-21</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/BigData/" itemprop="url" rel="index"><span itemprop="name">BigData</span></a>
                </span>
            </span>

          
            <span class="post-meta-item" title="Views" id="busuanzi_container_page_pv" style="display: none;">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">Views: </span>
              <span id="busuanzi_value_page_pv"></span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h2 id="Python处理各种Spark数据源"><a href="#Python处理各种Spark数据源" class="headerlink" title="Python处理各种Spark数据源"></a>Python处理各种Spark数据源</h2><p>Spark SQL通过DataFrame接口支持对各种数据源进行操作。DataFrame可以使用关系型数据库转换操作，也可以用来创建临时视图。将一个DataFrame注册为临时视图允许您对其数据运行SQL查询。现在本人介绍使用Spark加载和保存数据的基本使用方法，然后介绍内置数据源的读取和保存。</p>
<h3 id="数据的加载和保存"><a href="#数据的加载和保存" class="headerlink" title="数据的加载和保存"></a>数据的加载和保存</h3><ol>
<li>普通方式加载和保存spark默认数据类型parquet</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">df = spark.read.load(<span class="string">&quot;examples/src/main/resources/users.parquet&quot;</span>)</span><br><span class="line">df.select(<span class="string">&quot;name&quot;</span>, <span class="string">&quot;favorite_color&quot;</span>).write.save(<span class="string">&quot;namesAndFavColors.parquet&quot;</span>)</span><br></pre></td></tr></table></figure>
<ol start="2">
<li>在加载和保存时，通过参数指定数据源类型</li>
</ol>
<p>我们可以在保存和读取时指定要使用的数据源以及希望传递给数据源的其他参数。数据源由它们的全限定名，也可以使用它们的名称，比如json、parquet、jdbc、orc、libsvm、csv、text等后面将一一介绍。从任何数据源类型的数据都可以使用这种语法转换为其他类型。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">df = spark.read.load(<span class="string">&quot;examples/src/main/resources/people.json&quot;</span>, <span class="built_in">format</span>=<span class="string">&quot;json&quot;</span>)</span><br><span class="line">df.select(<span class="string">&quot;name&quot;</span>, <span class="string">&quot;age&quot;</span>).write.save(<span class="string">&quot;namesAndAges.parquet&quot;</span>, <span class="built_in">format</span>=<span class="string">&quot;parquet&quot;</span>)</span><br></pre></td></tr></table></figure>
<ol start="3">
<li>在读取文件的的时候直接运行SQL</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df = spark.sql(<span class="string">&quot;SELECT * FROM parquet.`examples/src/main/resources/users.parquet`&quot;</span>)</span><br></pre></td></tr></table></figure>
<a id="more"></a>
<ol start="4">
<li>保存数据的模式</li>
</ol>
<table>
<thead>
<tr>
<th>模式类型</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td>error(default)</td>
<td>如果保存数据的时候，数据已经存在，将会抛出错误说明</td>
</tr>
<tr>
<td>append</td>
<td>如果保存数据的时候，数据已经存在，将会把数据追加保存</td>
</tr>
<tr>
<td>overwrite</td>
<td>如果保存数据的时候，数据已经存在，将会把数据重新保存</td>
</tr>
<tr>
<td>ignore</td>
<td>如果保存数据的时候，数据已经存在，将不会对已有数据进行操作，就像SQL语句<em>CREATE TABLE IF NOT EXISTS</em></td>
</tr>
</tbody></table>
<ol start="5">
<li>保存到持久表</li>
</ol>
<p>我们还可以使用<strong>saveAsTable</strong>命令将<strong>DataFrames</strong>永久保存到<strong>Hive metastore</strong>中。大家一想到要保存的Hive中，就要部署一套Hive，其实使用此特性不需要现有的Hive部署。Spark将使用Derby本地自动创建一个Hive metastore。与<code>createOrReplaceTempView</code>命令不同的是<code>saveAsTable</code>将实现DataFrame的内容，并创建指向Hive metastore中的数据的指针。即使在Spark程序重新启动之后，持久表仍然存在。可以通过在带有表名的<code>SparkSession</code>上调用表方法来创建持久表的DataFrame。</p>
<p>对于基于文件的数据源，例如文本、parquet、json等，可以通过path选项指定自定义表路径，例如<code>df.write.option(&quot;path&quot;、&quot;/some/path&quot;).saveAsTable(&#39;t&#39;)</code>。删除表时，自定义表路径不会被删除，表数据仍然存在。如果没有指定自定义表路径，Spark将把数据写到仓库目录下的默认表路径。删除表时，默认的表路径也将被删除。</p>
<p>从Spark 2.1开始，持久数据源表的每个分区元数据存储在’Hive metastore’中。这带来了几个好处:</p>
<ul>
<li><p>由于转移点只能返回查询所需的分区，因此不再需要将第一个查询中的所有分区都发现到表中。</p>
</li>
<li><p>Hive DDLs例如<code>ALTER TABLE PARTITION ... SET LOCATION</code>现在对使用数据源API创建的表可用。</p>
</li>
</ul>
<p>注意，在创建外部数据源表(带有’path’选项的表)时，默认情况下不会收集分区信息。要在metastore同步分区信息，可以调用<code>MSCK REPAIR TABLE</code>。</p>
<ol start="6">
<li>嵌套、排序和分区</li>
</ol>
<p>对于基于文件的数据源，在保存时可以进行嵌套、排序或分区。但是套接和排序只适用于持久表:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df.write.bucketBy(<span class="number">42</span>, <span class="string">&quot;name&quot;</span>).sortBy(<span class="string">&quot;age&quot;</span>).saveAsTable(<span class="string">&quot;people_bucketed&quot;</span>)</span><br></pre></td></tr></table></figure>
<p>当使用数据集api时，分区可以与save和saveAsTable一起使用。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df.write.partitionBy(<span class="string">&quot;favorite_color&quot;</span>).<span class="built_in">format</span>(<span class="string">&quot;parquet&quot;</span>).save(<span class="string">&quot;namesPartByColor.parquet&quot;</span>)</span><br></pre></td></tr></table></figure>
<p>可以对单表同时使用分区和嵌套:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">df = spark.read.parquet(<span class="string">&quot;examples/src/main/resources/users.parquet&quot;</span>)</span><br><span class="line">(df</span><br><span class="line">    .write</span><br><span class="line">    .partitionBy(<span class="string">&quot;favorite_color&quot;</span>)</span><br><span class="line">    .bucketBy(<span class="number">42</span>, <span class="string">&quot;name&quot;</span>)</span><br><span class="line">    .saveAsTable(<span class="string">&quot;people_partitioned_bucketed&quot;</span>))</span><br></pre></td></tr></table></figure>
<p><strong>partitionBy</strong> 创建一个目录结构目录结构下面的有所介绍。因此，它对具有高基数的列的适用性有限。相比之下，<strong>bucketBy</strong> 将数据分布在固定数量的bucket上，并且可以在许多惟一值是无界的情况下使用。</p>
<h3 id="parquet文件"><a href="#parquet文件" class="headerlink" title="parquet文件"></a>parquet文件</h3><p>parquet是一种列式的格式，它受到许多其他数据处理系统的支持。Spark SQL支持读取和写入parquet文件，这些文件自动保存原始数据的模式。在保存parquet文件时，出于兼容性的原因，所有列都自动转换为可空列。关于parquet更多信息，请查看本人的<a href="https://javyxu.cn/bigdata/parquet/">Parquet简介</a>。</p>
<ol>
<li>使用Python加载数据</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">peopleDF = spark.read.json(<span class="string">&quot;examples/src/main/resources/people.json&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># DataFrames can be saved as Parquet files, maintaining the schema information.</span></span><br><span class="line">peopleDF.write.parquet(<span class="string">&quot;people.parquet&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Read in the Parquet file created above.</span></span><br><span class="line"><span class="comment"># Parquet files are self-describing so the schema is preserved.</span></span><br><span class="line"><span class="comment"># The result of loading a parquet file is also a DataFrame.</span></span><br><span class="line">parquetFile = spark.read.parquet(<span class="string">&quot;people.parquet&quot;</span>)</span><br><span class="line"></span><br><span class="line">parquetFile.createOrReplaceTempView(<span class="string">&quot;parquetFile&quot;</span>)</span><br><span class="line">teenagers = spark.sql(<span class="string">&quot;SELECT name FROM parquetFile WHERE age &gt;= 13 AND age &lt;= 19&quot;</span>)</span><br><span class="line">teenagers.show()</span><br></pre></td></tr></table></figure>
<ol start="2">
<li>探索分区</li>
</ol>
<p>表分区是Hive等系统中常用的优化方法。在分区表中，数据通常存储在不同的目录中，分区列值编码在每个分区目录的路径中。所有内置的文件源(包括text、CSV、JSON、ORC、Parquet)都能够自动发现和推断分区信息。例如，我们可以使用以下目录结构将所有以前使用的人口数据存储到分区表中，其中两个额外的列，<code>gender</code>和<code>country</code>作为分区列:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">path</span><br><span class="line">└── to</span><br><span class="line">    └── table</span><br><span class="line">        ├── gender&#x3D;male</span><br><span class="line">        │   ├── ...</span><br><span class="line">        │   │</span><br><span class="line">        │   ├── country&#x3D;US</span><br><span class="line">        │   │   └── data.parquet</span><br><span class="line">        │   ├── country&#x3D;CN</span><br><span class="line">        │   │   └── data.parquet</span><br><span class="line">        │   └── ...</span><br><span class="line">        └── gender&#x3D;female</span><br><span class="line">            ├── ...</span><br><span class="line">            │</span><br><span class="line">            ├── country&#x3D;US</span><br><span class="line">            │   └── data.parquet</span><br><span class="line">            ├── country&#x3D;CN</span><br><span class="line">            │   └── data.parquet</span><br><span class="line">            └── ...</span><br></pre></td></tr></table></figure>
<p>通过<code>path/to/table</code>使用<code>SparkSession.read.parquet``或SparkSession.read.load</code>，Spark SQL将自动从路径中提取分区信息。现在返回的DataFrame的模式变成:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">root</span><br><span class="line">|-- name: string (nullable &#x3D; true)</span><br><span class="line">|-- age: long (nullable &#x3D; true)</span><br><span class="line">|-- gender: string (nullable &#x3D; true)</span><br><span class="line">|-- country: string (nullable &#x3D; true)</span><br></pre></td></tr></table></figure>
<p>注意，分区列的数据类型是自动推断的。目前，支持数字数据类型、日期、时间戳和字符串类型。有时用户可能不希望自动推断分区列的数据类型。对于这些用例，可以通过<code>spark.sql.sources.partitionColumnTypeInference.enabled</code>自动推断类型，默认为true。当为false时，分区列将使用string类型。</p>
<p>从Spark 1.6.0开始，默认情况下，分区发现只能在给定路径下找到分区。对于上面的例子，如果用户将path/to/table/gender=male传递给任何一个<code>SparkSession.read.parquet</code>或<code>SparkSession.read.load</code>时，<code>gender</code>将不会视为分区列。如果用户需要指定分区发现应该开始的基本路径，他们可以在数据源选项中设置basePath。例如，当path/to/table/gender=male是数据的路径，用户将basePath设置为path/to/table/时，gender将是一个分区列。</p>
<h4 id="模式合并"><a href="#模式合并" class="headerlink" title="模式合并"></a>模式合并</h4><p>像ProtocolBuffer、Avro和Thrift一样，Parquet也支持模式演化。用户可以从一个简单的模式开始，然后根据需要逐渐向该模式添加更多的列。这样，用户可能会得到多个具有不同但相互兼容的模式的Parquet文件。Parquet数据源现在能够自动检测这种情况并合并所有这些文件的模式。</p>
<p>由于模式合并是一个相对昂贵的操作，而且在大多数情况下不是必需的，因此我们从1.5.0开始默认关闭它。你可以通过</p>
<ol>
<li><p>读取parquet文件时，将数据源选项<code>mergeSchema</code>设置为<code>true</code>(如下面的示例所示)，或</p>
</li>
<li><p>设置全局SQL选项<code>spark.sql.parquet.mergeSchema</code>为<code>true</code>。</p>
</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> Row</span><br><span class="line"></span><br><span class="line"><span class="comment"># spark is from the previous example.</span></span><br><span class="line"><span class="comment"># Create a simple DataFrame, stored into a partition directory</span></span><br><span class="line">sc = spark.sparkContext</span><br><span class="line"></span><br><span class="line">squaresDF = spark.createDataFrame(sc.parallelize(<span class="built_in">range</span>(<span class="number">1</span>, <span class="number">6</span>))</span><br><span class="line">                                  .<span class="built_in">map</span>(<span class="keyword">lambda</span> i: Row(single=i, double=i ** <span class="number">2</span>)))</span><br><span class="line">squaresDF.write.parquet(<span class="string">&quot;data/test_table/key=1&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Create another DataFrame in a new partition directory,</span></span><br><span class="line"><span class="comment"># adding a new column and dropping an existing column</span></span><br><span class="line">cubesDF = spark.createDataFrame(sc.parallelize(<span class="built_in">range</span>(<span class="number">6</span>, <span class="number">11</span>))</span><br><span class="line">                                .<span class="built_in">map</span>(<span class="keyword">lambda</span> i: Row(single=i, triple=i ** <span class="number">3</span>)))</span><br><span class="line">cubesDF.write.parquet(<span class="string">&quot;data/test_table/key=2&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Read the partitioned table</span></span><br><span class="line">mergedDF = spark.read.option(<span class="string">&quot;mergeSchema&quot;</span>, <span class="string">&quot;true&quot;</span>).parquet(<span class="string">&quot;data/test_table&quot;</span>)</span><br><span class="line">mergedDF.printSchema()</span><br><span class="line"></span><br><span class="line"><span class="comment"># The final schema consists of all 3 columns in the Parquet files together</span></span><br><span class="line"><span class="comment"># with the partitioning column appeared in the partition directory paths.</span></span><br><span class="line"><span class="comment"># root</span></span><br><span class="line"><span class="comment">#  |-- double: long (nullable = true)</span></span><br><span class="line"><span class="comment">#  |-- single: long (nullable = true)</span></span><br><span class="line"><span class="comment">#  |-- triple: long (nullable = true)</span></span><br><span class="line"><span class="comment">#  |-- key: integer (nullable = true)</span></span><br></pre></td></tr></table></figure>
<h4 id="Hive-metastore转换为parquet表"><a href="#Hive-metastore转换为parquet表" class="headerlink" title="Hive metastore转换为parquet表"></a>Hive metastore转换为parquet表</h4><p>在从Hive metastore parquet table读取和写入数据时，Spark SQL将尝试使用自己的parquet，而不是Hive SerDe，以获得更好的性能。这个行为由<code>spark.sql.hive.convertMetastoreParquet</code>配置，并在默认情况下打开。</p>
<h5 id="Hive-Parquet-Schema协调"><a href="#Hive-Parquet-Schema协调" class="headerlink" title="Hive/Parquet Schema协调"></a>Hive/Parquet Schema协调</h5><p>从表模式处理的角度来看，Hive和Parquet有两个关键的区别。</p>
<ol>
<li><p>Hive是不区分大小写的，而parquet区分。</p>
</li>
<li><p>Hive认为所有的列都是可空的，而在parquet中的可空性是很重要的</p>
</li>
</ol>
<p>因此，在将<code>Hive metastore parquet table</code>转换为<code>Spark SQL parquet table</code>时，我们必须将<code>Hive metastore parquet table</code>与<code>Parquet</code>模式进行协调。协调规则如下:</p>
<ol>
<li><p>无论是否为空，两个模式中具有相同名称的字段必须具有相同的数据类型。协调字段应该具有parquet的数据类型，以便考虑可空性。</p>
</li>
<li><p>协调模式正好包含在Hive metastore模式中定义的字段。</p>
</li>
</ol>
<ul>
<li><p>在parquet模式中的任何字段都将被删除在协调模式中。</p>
</li>
<li><p>只出现在Hive metastore模式中的任何字段都作为可空字段添加到协调模式中。</p>
</li>
</ul>
<h5 id="元数据刷新"><a href="#元数据刷新" class="headerlink" title="元数据刷新"></a>元数据刷新</h5><p>Spark SQL缓存parquet元数据以获得更好的性能。当启用Hive metastore parquet table转换时，这些转换表的元数据也会被缓存。如果这些表是由Hive或其他外部工具更新的，则需要手动刷新它们，以确保一致的元数据。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># spark is an existing SparkSession</span></span><br><span class="line">spark.catalog.refreshTable(<span class="string">&quot;my_table&quot;</span>)</span><br></pre></td></tr></table></figure>
<h4 id="配置"><a href="#配置" class="headerlink" title="配置"></a>配置</h4><p>可以使用SparkSession上的<code>setConf</code>方法或使用SQL运行<code>SET key=value</code>命令来配置parquet。</p>
<table>
<thead>
<tr>
<th>属性名称</th>
<th>默认值</th>
<th>描述</th>
</tr>
</thead>
<tbody><tr>
<td>spark.sql.parquet.binaryAsString</td>
<td>false</td>
<td>其他一些生成Parquet的系统，特别是Impala、Hive和Spark SQL的旧版本，在编写Parquet模式时不区分二进制数据和字符串。这个标志告诉Spark SQL将二进制数据解释为字符串，以提供与这些系统的兼容性。</td>
</tr>
<tr>
<td>spark.sql.parquet.int96AsTimestamp</td>
<td>true</td>
<td>一些parquet生产系统，特别是Impala和Hive，将时间戳存储到INT96中。这个标志告诉Spark SQL将INT96数据解释为一个时间戳，以提供与这些系统的兼容性。</td>
</tr>
<tr>
<td>spark.sql.parquet.cacheMetadata</td>
<td>true</td>
<td>打开parquet模式元数据的缓存。可以加速静态数据的查询。snappy设置编写拼花文件时使用的压缩编解码器。可接受的值包括:uncompression、snappy、gzip、lzo。</td>
</tr>
<tr>
<td>spark.sql.parquet.filterPushdown</td>
<td>true</td>
<td>设置为true时，启用parquet过滤下推优化</td>
</tr>
<tr>
<td>spark.sql.hive.convertMetastoreParquet</td>
<td>true</td>
<td>当设置为false时，Spark SQL将对拼花表使用Hive SerDe，而不是内置支持。</td>
</tr>
<tr>
<td>spark.sql.parquet.mergeSchema</td>
<td>false</td>
<td>如果为真，Parquet数据源将合并从所有数据文件收集的模式，否则将从总结文件中选择模式，如果没有总结文件可用，则从随机数据文件中选择模式。</td>
</tr>
<tr>
<td>spark.sql.optimizer.metadataOnly</td>
<td>true</td>
<td>如果为真，则启用仅使用表的元数据来生成分区列而不是表扫描的元数据查询优化。当扫描的所有列都是分区列且查询具有满足不同语义的聚合操作符时，该方法将适用。</td>
</tr>
</tbody></table>
<h3 id="json数据集"><a href="#json数据集" class="headerlink" title="json数据集"></a>json数据集</h3><p>Spark SQL可以自动推断JSON数据集的模式，并将其作为DataFrame加载。可以使用<code>SparkSession.read.json</code>读取json文件。</p>
<p>注意，作为json文件提供的文件不是典型的json文件。每行必须包含一个单独的、自包含的有效JSON对象。有关更多信息请查看<a target="_blank" rel="noopener" href="http://jsonlines.org/">JSON行文本格式，也称为以新行分隔的JSON</a></p>
<p>对于常规的多行JSON文件，将多行参数设置为True。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># spark is from the previous example.</span></span><br><span class="line">sc = spark.sparkContext</span><br><span class="line"></span><br><span class="line"><span class="comment"># A JSON dataset is pointed to by path.</span></span><br><span class="line"><span class="comment"># The path can be either a single text file or a directory storing text files</span></span><br><span class="line">path = <span class="string">&quot;examples/src/main/resources/people.json&quot;</span></span><br><span class="line">peopleDF = spark.read.json(path)</span><br><span class="line"></span><br><span class="line"><span class="comment"># The inferred schema can be visualized using the printSchema() method</span></span><br><span class="line">peopleDF.printSchema()</span><br><span class="line"><span class="comment"># root</span></span><br><span class="line"><span class="comment">#  |-- age: long (nullable = true)</span></span><br><span class="line"><span class="comment">#  |-- name: string (nullable = true)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Creates a temporary view using the DataFrame</span></span><br><span class="line">peopleDF.createOrReplaceTempView(<span class="string">&quot;people&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># SQL statements can be run by using the sql methods provided by spark</span></span><br><span class="line">teenagerNamesDF = spark.sql(<span class="string">&quot;SELECT name FROM people WHERE age BETWEEN 13 AND 19&quot;</span>)</span><br><span class="line">teenagerNamesDF.show()</span><br><span class="line"><span class="comment"># +------+</span></span><br><span class="line"><span class="comment"># |  name|</span></span><br><span class="line"><span class="comment"># +------+</span></span><br><span class="line"><span class="comment"># |Justin|</span></span><br><span class="line"><span class="comment"># +------+</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Alternatively, a DataFrame can be created for a JSON dataset represented by</span></span><br><span class="line"><span class="comment"># an RDD[String] storing one JSON object per string</span></span><br><span class="line">jsonStrings = [<span class="string">&#x27;&#123;&quot;name&quot;:&quot;Yin&quot;,&quot;address&quot;:&#123;&quot;city&quot;:&quot;Columbus&quot;,&quot;state&quot;:&quot;Ohio&quot;&#125;&#125;&#x27;</span>]</span><br><span class="line">otherPeopleRDD = sc.parallelize(jsonStrings)</span><br><span class="line">otherPeople = spark.read.json(otherPeopleRDD)</span><br><span class="line">otherPeople.show()</span><br><span class="line"><span class="comment"># +---------------+----+</span></span><br><span class="line"><span class="comment"># |        address|name|</span></span><br><span class="line"><span class="comment"># +---------------+----+</span></span><br><span class="line"><span class="comment"># |[Columbus,Ohio]| Yin|</span></span><br><span class="line"><span class="comment"># +---------------+----+</span></span><br></pre></td></tr></table></figure>
<h3 id="Hive表"><a href="#Hive表" class="headerlink" title="Hive表"></a>Hive表</h3><p>Spark SQL还支持读取和写入存储在<a target="_blank" rel="noopener" href="http://hive.apache.org/">Apache Hive</a>中的数据。然而，由于Hive有大量的依赖项，这些依赖项并不包含在默认的Spark发行版中。如果可以在类路径中找到Hive依赖项，Spark将自动加载它们。请注意，这些Hive依赖项还必须出现在所有工作节点上，因为它们需要访问Hive序列化和反序列化库(SerDes)，以便访问存储在Hive中的数据。</p>
<p>Hive的配置是通过放置Hive站点来完成的: 配置在<code>conf</code>下的hive-site.xml, core-site.xml(用于安全配置)和hdfs-site.conf(用于HDFS配置)文件。</p>
<p>在使用Hive时，必须使用Hive支持实例化<code>SparkSession</code>，包括连接到持久的Hive转移、支持Hive serdes和Hive user-defined的函数。用户仍然可以启用Hive支持在没有Hive部署的机器上。当没有配置<code>hive-site.xml</code>时。上下文会在当前目录中自动创建metastore_db，并创建一个由<code>spark.sql.warehouse.dir</code>。默认为启动Spark应用程序的当前目录中的Spark-warehouse目录。注意<code>hive.metastore.warehouse.dir</code>从Spark 2.0.0开始，<code>hive-site.xml</code>就被弃用了。相反,使用<code>spark.sql.warehouse.dir</code>指定数据库在仓库中的默认位置。您可能需要将写权限授予启动Spark应用程序的用户。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">from</span> os.path <span class="keyword">import</span> expanduser, join, abspath</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> SparkSession</span><br><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> Row</span><br><span class="line"></span><br><span class="line"><span class="comment"># warehouse_location points to the default location for managed databases and tables</span></span><br><span class="line">warehouse_location = abspath(<span class="string">&#x27;spark-warehouse&#x27;</span>)</span><br><span class="line"></span><br><span class="line">spark = SparkSession \</span><br><span class="line">    .builder \</span><br><span class="line">    .appName(<span class="string">&quot;Python Spark SQL Hive integration example&quot;</span>) \</span><br><span class="line">    .config(<span class="string">&quot;spark.sql.warehouse.dir&quot;</span>, warehouse_location) \</span><br><span class="line">    .enableHiveSupport() \</span><br><span class="line">    .getOrCreate()</span><br><span class="line"></span><br><span class="line"><span class="comment"># spark is an existing SparkSession</span></span><br><span class="line">spark.sql(<span class="string">&quot;CREATE TABLE IF NOT EXISTS src (key INT, value STRING) USING hive&quot;</span>)</span><br><span class="line">spark.sql(<span class="string">&quot;LOAD DATA LOCAL INPATH &#x27;examples/src/main/resources/kv1.txt&#x27; INTO TABLE src&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Queries are expressed in HiveQL</span></span><br><span class="line">spark.sql(<span class="string">&quot;SELECT * FROM src&quot;</span>).show()</span><br><span class="line"><span class="comment"># +---+-------+</span></span><br><span class="line"><span class="comment"># |key|  value|</span></span><br><span class="line"><span class="comment"># +---+-------+</span></span><br><span class="line"><span class="comment"># |238|val_238|</span></span><br><span class="line"><span class="comment"># | 86| val_86|</span></span><br><span class="line"><span class="comment"># |311|val_311|</span></span><br><span class="line"><span class="comment"># ...</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Aggregation queries are also supported.</span></span><br><span class="line">spark.sql(<span class="string">&quot;SELECT COUNT(*) FROM src&quot;</span>).show()</span><br><span class="line"><span class="comment"># +--------+</span></span><br><span class="line"><span class="comment"># |count(1)|</span></span><br><span class="line"><span class="comment"># +--------+</span></span><br><span class="line"><span class="comment"># |    500 |</span></span><br><span class="line"><span class="comment"># +--------+</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># The results of SQL queries are themselves DataFrames and support all normal functions.</span></span><br><span class="line">sqlDF = spark.sql(<span class="string">&quot;SELECT key, value FROM src WHERE key &lt; 10 ORDER BY key&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># The items in DataFrames are of type Row, which allows you to access each column by ordinal.</span></span><br><span class="line">stringsDS = sqlDF.rdd.<span class="built_in">map</span>(<span class="keyword">lambda</span> row: <span class="string">&quot;Key: %d, Value: %s&quot;</span> % (row.key, row.value))</span><br><span class="line"><span class="keyword">for</span> record <span class="keyword">in</span> stringsDS.collect():</span><br><span class="line">    print(record)</span><br><span class="line"><span class="comment"># Key: 0, Value: val_0</span></span><br><span class="line"><span class="comment"># Key: 0, Value: val_0</span></span><br><span class="line"><span class="comment"># Key: 0, Value: val_0</span></span><br><span class="line"><span class="comment"># ...</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># You can also use DataFrames to create temporary views within a SparkSession.</span></span><br><span class="line">Record = Row(<span class="string">&quot;key&quot;</span>, <span class="string">&quot;value&quot;</span>)</span><br><span class="line">recordsDF = spark.createDataFrame([Record(i, <span class="string">&quot;val_&quot;</span> + <span class="built_in">str</span>(i)) <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, <span class="number">101</span>)])</span><br><span class="line">recordsDF.createOrReplaceTempView(<span class="string">&quot;records&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Queries can then join DataFrame data with data stored in Hive.</span></span><br><span class="line">spark.sql(<span class="string">&quot;SELECT * FROM records r JOIN src s ON r.key = s.key&quot;</span>).show()</span><br><span class="line"><span class="comment"># +---+------+---+------+</span></span><br><span class="line"><span class="comment"># |key| value|key| value|</span></span><br><span class="line"><span class="comment"># +---+------+---+------+</span></span><br><span class="line"><span class="comment"># |  2| val_2|  2| val_2|</span></span><br><span class="line"><span class="comment"># |  4| val_4|  4| val_4|</span></span><br><span class="line"><span class="comment"># |  5| val_5|  5| val_5|</span></span><br><span class="line"><span class="comment"># ...</span></span><br></pre></td></tr></table></figure>
<h4 id="指定Hive表的存储格式"><a href="#指定Hive表的存储格式" class="headerlink" title="指定Hive表的存储格式"></a>指定Hive表的存储格式</h4><p>在创建Hive表时，需要定义这个表应该如何向文件系统读写数据，即”输入格式”和”输出格式”。您还需要定义这个表应该如何将数据反序列化为行，或将行序列化为数据，即”serde”。以下选项可用于指定存储格式(“serde”、”input format”、”output format”)，例如<code>CREATE TABLE src(id int) USING hive OPTIONS(fileFormat &#39;parquet&#39;).</code>。默认情况下，我们将以纯文本的形式读取表文件。注意，在创建表时还不支持Hive存储处理程序，您可以使用Hive侧的存储处理程序创建表，并使用Spark SQL读取它。</p>
<table>
<thead>
<tr>
<th>属性名</th>
<th>描述</th>
</tr>
</thead>
<tbody><tr>
<td>fileFormat</td>
<td>fileFormat是一种存储格式规范的包，包括”serde”、”input format”和”output format”。目前我们支持6种文件格式:”sequencefile”，”rcfile”，”orc”，”parquet”，”textfile”和”avro”。</td>
</tr>
<tr>
<td>inputFormat, outputFormat</td>
<td>这两个选项将对应的”InputFormat”和”OutputFormat”类的名称指定为字符串文本，例如。”org.apache.hadoop.hive.ql.io.orc.OrcInputFormat”。这两个选项必须成对出现，如果已经指定了”fileFormat”选项，则不能指定它们。</td>
</tr>
<tr>
<td>serde</td>
<td>此选项指定serde类的名称。指定”fileFormat”选项时，如果给定的”fileFormat”已经包含serde的信息，则不要指定此选项。目前”sequencefile”、”textfile”和”rcfile”不包含serde信息，您可以在这三种文件格式中使用此选项。</td>
</tr>
<tr>
<td>fieldDelim, escapeDelim, collectionDelim, mapkeyDelim, lineDelim</td>
<td>此选项只能与”textfile”文件格式一起使用。它们定义如何将带分隔符的文件读入行。</td>
</tr>
</tbody></table>
<p>使用<code>OPTIONS</code>定义的所有其他属性将被视为Hive serde属性。</p>
<h4 id="与不同版本的Hive-Metastore相互作用"><a href="#与不同版本的Hive-Metastore相互作用" class="headerlink" title="与不同版本的Hive Metastore相互作用"></a>与不同版本的Hive Metastore相互作用</h4><p>Spark SQL的Hive支持中最重要的部分之一是与Hive metastore的交互，这使得Spark SQL能够访问Hive表的元数据。从Spark 1.4.0开始，Spark SQL的一个二进制构建可以使用下面描述的配置来查询不同版本的Hive metastores。注意，独立于用于与metastore通信的Hive版本，内部Spark SQL将针对Hive 1.2.1编译，并将这些类用于内部执行(serdes、udf、udf等)。</p>
<p>以下选项可用于配置用于检索元数据的Hive版本:</p>
<table>
<thead>
<tr>
<th>属性名称</th>
<th>默认值</th>
<th>描述</th>
</tr>
</thead>
<tbody><tr>
<td>spark.sql.hive.metastore.version</td>
<td>1.2.1</td>
<td>Hive metastore版本号。 蜂巢转移。可用选项从0.12.0到1.2.1</td>
</tr>
<tr>
<td>spark.sql.hive.metastore.jars</td>
<td>builtin</td>
<td>用于实例化HiveMetastoreClient的jar的位置。此属性可以是三个选项之一:<strong>builtin：</strong> 使用Hive 1.2.1，当启用-Phive时，它与Spark程序集绑定在一起。当选择此选项时，spark.sq .hive.metastore。版本必须是1.2.1或未定义。<strong>maven：</strong> 用从Maven存储库下载的指定版本的Hive jar。一般不建议将此配置用于生产部署。 JVM标准格式的类路径。这个类路径必须包括所有Hive及其依赖项，包括正确的Hadoop版本。这些jar只需要出现在驱动程序上，但是如果您在yarn集群模式下运行，那么您必须确保它们与您的应用程序一起打包。</td>
</tr>
<tr>
<td>spark.sql.hive.metastore.sharedPrefixes</td>
<td>com.mysql.jdbc, org.postgresql, com.microsoft.sqlserver, oracle.jdbc</td>
<td>类前缀的逗号分隔列表，应该使用在Spark SQL和特定版本的Hive之间共享的类加载器加载这些前缀。应该共享的类的一个例子是JDBC驱动程序，它需要与转移服务器进行通信。需要共享的其他类是那些与已经共享的类交互的类。例如，log4j使用的自定义appender。</td>
</tr>
<tr>
<td>spark.sql.hive.metastore.barrierPrefixes</td>
<td>(empty)</td>
<td>Spark SQL正在与之通信的每个Hive版本都应该显式地重新加载类前缀的逗号分隔列表。例如，Hive udf声明在一个通常会被共享的前缀中(即org.apache.spark.*)。</td>
</tr>
</tbody></table>
<h3 id="通过JDBC来连接其他的关系型数据库"><a href="#通过JDBC来连接其他的关系型数据库" class="headerlink" title="通过JDBC来连接其他的关系型数据库"></a>通过JDBC来连接其他的关系型数据库</h3><p>Spark SQL还包括一个数据源，可以使用JDBC从其他数据库读取数据。与使用<a target="_blank" rel="noopener" href="https://spark.apache.org/docs/2.2.2/api/scala/index.html#org.apache.spark.rdd.JdbcRDD">JdbcRDD</a>相比，这种功能更可取。这是因为结果是以DataFrame的形式返回的，并且它们可以在Spark SQL中轻松处理或与其他数据源连接。JDBC数据源也更容易从Java或Python中使用，因为它不需要用户提供一个ClassTag。(请注意，这与Spark SQL JDBC服务器不同，后者允许其他应用程序使用Spark SQL运行查询)。</p>
<p>首先，您需要在spark类路径中包含特定数据库的JDBC驱动程序。例如，要从Spark Shell连接到postgres,对于postgres的按照查看<a href="https://javyxu.cn/postgresql/install_postgres/">此处</a>，需要运行以下命令:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin&#x2F;spark-shell --driver-class-path postgresql-9.4.1207.jar --jars postgresql-9.4.1207.jar</span><br></pre></td></tr></table></figure>
<p>可以使用数据源API将远程数据库中的表加载为DataFrame或Spark SQL临时视图。用户可以在数据源选项中指定JDBC连接属性。用户和密码通常作为登录数据源的连接属性提供。除了连接属性，Spark还支持以下不区分大小写的选项:</p>
<table>
<thead>
<tr>
<th>属性名</th>
<th>描述</th>
</tr>
</thead>
<tbody><tr>
<td>url</td>
<td>要连接的JDBC URL。特定于源代码的连接属性可以在URL中指定。例如： <code>jdbc:postgresql://localhost/test?user=fred&amp;password=secret</code></td>
</tr>
<tr>
<td>dbtable</td>
<td>应该读取的JDBC表。注意，可以使用SQL查询的FROM子句中有效的任何内容。例如，您也可以使用圆括号中的子查询来代替完整的表。</td>
</tr>
<tr>
<td>driver</td>
<td>用于连接此URL的JDBC驱动程序的类名。</td>
</tr>
<tr>
<td>partitionColumn, lowerBound, upperBound</td>
<td>如果指定了这些选项中的任何一个，则必须指定所有这些选项。此外，必须指定numpartition。它们描述了当从多个worker并行读取数据时如何对表进行分区。partitionColumn必须是表中的数字列。注意，下界和上界仅用于决定分区步幅，而不是用于过滤表中的行。因此，表中的所有行都将被分区并返回。此选项仅适用于阅读。</td>
</tr>
<tr>
<td>numPartitions</td>
<td>用于表读写并行性的分区的最大数目。这也决定了并发JDBC连接的最大数量。如果要写入的分区数量超过此限制，则在写入之前调用coalesce(numPartitions)将其减少到此限制。</td>
</tr>
<tr>
<td>fetchsize</td>
<td>JDBC获取大小，它决定每次往返要获取多少行。这有助于JDBC驱动程序的性能，JDBC驱动程序默认为低获取大小(例如。Oracle有10行)。此选项仅适用于阅读。</td>
</tr>
<tr>
<td>batchsize</td>
<td>JDBC批处理大小，它决定每个往返要插入多少行。这可以帮助JDBC驱动程序的性能。此选项仅适用于写入。默认值是1000。</td>
</tr>
<tr>
<td>isolationLevel</td>
<td>事务隔离级别，适用于当前连接。它可以是NONE、READ_COMMITTED、READ_UNCOMMITTED、REPEATABLE_READ或SERIALIZABLE之一，对应于JDBC的连接对象定义的标准事务隔离级别，默认为READ_UNCOMMITTED。此选项仅适用于写入。请参考java.sql.Connection中的文档。</td>
</tr>
<tr>
<td>truncate</td>
<td>这是一个JDBC编写器相关的选项。当SaveMode。启用覆盖后，此选项将导致Spark截断现有表，而不是删除和重新创建该表。这可以更有效地防止表元数据(例如索引)被删除。但是，在某些情况下，例如新数据具有不同的模式时，它将不起作用。它默认为false。此选项仅适用于写入。</td>
</tr>
<tr>
<td>createTableOptions</td>
<td>这是一个JDBC编写器相关的选项。如果指定，该选项允许在创建表时设置特定于数据库的表和分区选项(例如，CREATE table t (name string) ENGINE=InnoDB.)。此选项仅适用于写入。</td>
</tr>
<tr>
<td>createTableColumnTypes</td>
<td>创建表时要使用的数据库列数据类型，而不是默认值。数据类型信息应该以与CREATE TABLE columns语法(e。g:”name CHAR(64)， comments VARCHAR(1024)”)。指定的类型应该是有效的spark sql数据类型。此选项仅适用于写入。</td>
</tr>
</tbody></table>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Note: JDBC loading and saving can be achieved via either the load/save or jdbc methods</span></span><br><span class="line"><span class="comment"># Loading data from a JDBC source</span></span><br><span class="line">jdbcDF = spark.read \</span><br><span class="line">    .<span class="built_in">format</span>(<span class="string">&quot;jdbc&quot;</span>) \</span><br><span class="line">    .option(<span class="string">&quot;url&quot;</span>, <span class="string">&quot;jdbc:postgresql:dbserver&quot;</span>) \</span><br><span class="line">    .option(<span class="string">&quot;dbtable&quot;</span>, <span class="string">&quot;schema.tablename&quot;</span>) \</span><br><span class="line">    .option(<span class="string">&quot;user&quot;</span>, <span class="string">&quot;username&quot;</span>) \</span><br><span class="line">    .option(<span class="string">&quot;password&quot;</span>, <span class="string">&quot;password&quot;</span>) \</span><br><span class="line">    .load()</span><br><span class="line"></span><br><span class="line">jdbcDF2 = spark.read \</span><br><span class="line">    .jdbc(<span class="string">&quot;jdbc:postgresql:dbserver&quot;</span>, <span class="string">&quot;schema.tablename&quot;</span>,</span><br><span class="line">          properties=&#123;<span class="string">&quot;user&quot;</span>: <span class="string">&quot;username&quot;</span>, <span class="string">&quot;password&quot;</span>: <span class="string">&quot;password&quot;</span>&#125;)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Saving data to a JDBC source</span></span><br><span class="line">jdbcDF.write \</span><br><span class="line">    .<span class="built_in">format</span>(<span class="string">&quot;jdbc&quot;</span>) \</span><br><span class="line">    .option(<span class="string">&quot;url&quot;</span>, <span class="string">&quot;jdbc:postgresql:dbserver&quot;</span>) \</span><br><span class="line">    .option(<span class="string">&quot;dbtable&quot;</span>, <span class="string">&quot;schema.tablename&quot;</span>) \</span><br><span class="line">    .option(<span class="string">&quot;user&quot;</span>, <span class="string">&quot;username&quot;</span>) \</span><br><span class="line">    .option(<span class="string">&quot;password&quot;</span>, <span class="string">&quot;password&quot;</span>) \</span><br><span class="line">    .save()</span><br><span class="line"></span><br><span class="line">jdbcDF2.write \</span><br><span class="line">    .jdbc(<span class="string">&quot;jdbc:postgresql:dbserver&quot;</span>, <span class="string">&quot;schema.tablename&quot;</span>,</span><br><span class="line">          properties=&#123;<span class="string">&quot;user&quot;</span>: <span class="string">&quot;username&quot;</span>, <span class="string">&quot;password&quot;</span>: <span class="string">&quot;password&quot;</span>&#125;)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Specifying create table column data types on write</span></span><br><span class="line">jdbcDF.write \</span><br><span class="line">    .option(<span class="string">&quot;createTableColumnTypes&quot;</span>, <span class="string">&quot;name CHAR(64), comments VARCHAR(1024)&quot;</span>) \</span><br><span class="line">    .jdbc(<span class="string">&quot;jdbc:postgresql:dbserver&quot;</span>, <span class="string">&quot;schema.tablename&quot;</span>,</span><br><span class="line">          properties=&#123;<span class="string">&quot;user&quot;</span>: <span class="string">&quot;username&quot;</span>, <span class="string">&quot;password&quot;</span>: <span class="string">&quot;password&quot;</span>&#125;)</span><br></pre></td></tr></table></figure>
<p><em>在Spark repo的”examples/src/main/python/sql/datasource.py”中找到完整的示例</em>。</p>
<h4 id="故障排除"><a href="#故障排除" class="headerlink" title="故障排除"></a>故障排除</h4><ul>
<li><p>JDBC驱动程序类必须对客户机会话和所有执行程序上的原始类装入器可见。这是因为Java的DriverManager类做了一个安全检查，当打开一个连接时，它会忽略所有原始类装入器不可见的驱动程序。一种方便的方法是修改compute_classpath。在所有工作节点上使用sh来包含驱动程序jar。</p>
</li>
<li><p>有些数据库，如H2，将所有名称转换为大写。您需要使用大写字母来引用Spark SQL中的这些名称。</p>
</li>
</ul>
<p><strong>要想了解更多关于Spark的使用，请查看本人下面博客：</strong></p>
<ol>
<li><a href="https://javyxu.cn/bigdata/using_spark/">Spark的使用</a></li>
<li><a href="https://javyxu.cn/bigdata/spark_datasources/">Python处理各种Spark数据源</a></li>
<li><a href="https://javyxu.cn/bigdata/spark_performance/">Spark性能优化</a></li>
</ol>

    </div>

    
    
    
        <div class="reward-container">
  <div>原创技术分享，您的支持将鼓励我继续创作</div>
  <button onclick="var qr = document.getElementById('qr'); qr.style.display = (qr.style.display === 'none') ? 'block' : 'none';">
    Donate
  </button>
  <div id="qr" style="display: none;">
      
      <div style="display: inline-block;">
        <img src="/images/wechatpay.png" alt="Javy WeChat Pay">
        <p>WeChat Pay</p>
      </div>
      
      <div style="display: inline-block;">
        <img src="/images/alipay.png" alt="Javy Alipay">
        <p>Alipay</p>
      </div>

  </div>
</div>


      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/BigData/" rel="tag"># BigData</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/python/python_optimizing_performance/" rel="prev" title="Python性能优化建议">
      <i class="fa fa-chevron-left"></i> Python性能优化建议
    </a></div>
      <div class="post-nav-item">
    <a href="/bigdata/spark_performance/" rel="next" title="Spark性能优化">
      Spark性能优化 <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          
    <div class="comments" id="gitalk-container"></div>

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#Python%E5%A4%84%E7%90%86%E5%90%84%E7%A7%8DSpark%E6%95%B0%E6%8D%AE%E6%BA%90"><span class="nav-number">1.</span> <span class="nav-text">Python处理各种Spark数据源</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%95%B0%E6%8D%AE%E7%9A%84%E5%8A%A0%E8%BD%BD%E5%92%8C%E4%BF%9D%E5%AD%98"><span class="nav-number">1.1.</span> <span class="nav-text">数据的加载和保存</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#parquet%E6%96%87%E4%BB%B6"><span class="nav-number">1.2.</span> <span class="nav-text">parquet文件</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%A8%A1%E5%BC%8F%E5%90%88%E5%B9%B6"><span class="nav-number">1.2.1.</span> <span class="nav-text">模式合并</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Hive-metastore%E8%BD%AC%E6%8D%A2%E4%B8%BAparquet%E8%A1%A8"><span class="nav-number">1.2.2.</span> <span class="nav-text">Hive metastore转换为parquet表</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#Hive-Parquet-Schema%E5%8D%8F%E8%B0%83"><span class="nav-number">1.2.2.1.</span> <span class="nav-text">Hive&#x2F;Parquet Schema协调</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E5%85%83%E6%95%B0%E6%8D%AE%E5%88%B7%E6%96%B0"><span class="nav-number">1.2.2.2.</span> <span class="nav-text">元数据刷新</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E9%85%8D%E7%BD%AE"><span class="nav-number">1.2.3.</span> <span class="nav-text">配置</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#json%E6%95%B0%E6%8D%AE%E9%9B%86"><span class="nav-number">1.3.</span> <span class="nav-text">json数据集</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Hive%E8%A1%A8"><span class="nav-number">1.4.</span> <span class="nav-text">Hive表</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%8C%87%E5%AE%9AHive%E8%A1%A8%E7%9A%84%E5%AD%98%E5%82%A8%E6%A0%BC%E5%BC%8F"><span class="nav-number">1.4.1.</span> <span class="nav-text">指定Hive表的存储格式</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E4%B8%8E%E4%B8%8D%E5%90%8C%E7%89%88%E6%9C%AC%E7%9A%84Hive-Metastore%E7%9B%B8%E4%BA%92%E4%BD%9C%E7%94%A8"><span class="nav-number">1.4.2.</span> <span class="nav-text">与不同版本的Hive Metastore相互作用</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%80%9A%E8%BF%87JDBC%E6%9D%A5%E8%BF%9E%E6%8E%A5%E5%85%B6%E4%BB%96%E7%9A%84%E5%85%B3%E7%B3%BB%E5%9E%8B%E6%95%B0%E6%8D%AE%E5%BA%93"><span class="nav-number">1.5.</span> <span class="nav-text">通过JDBC来连接其他的关系型数据库</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%95%85%E9%9A%9C%E6%8E%92%E9%99%A4"><span class="nav-number">1.5.1.</span> <span class="nav-text">故障排除</span></a></li></ol></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Javy"
      src="/images/avatar.jpg">
  <p class="site-author-name" itemprop="name">Javy</p>
  <div class="site-description" itemprop="description">You can create art and beauty on a computer.</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">74</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">13</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">30</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/javyxu" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;javyxu" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
  </div>


      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2018 - 2021</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Javy</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a>
  </div>

        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <span class="post-meta-item" id="busuanzi_container_site_uv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="Total Visitors">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item" id="busuanzi_container_site_pv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="Total Views">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  




  
<script src="/js/local-search.js"></script>













  

  

<link rel="stylesheet" href="//cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.css">

<script>
NexT.utils.loadComments(document.querySelector('#gitalk-container'), () => {
  NexT.utils.getScript('//cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.js', () => {
    var gitalk = new Gitalk({
      clientID    : 'de44f4e91f73c590197c',
      clientSecret: '840b907d657247c9ddcbcec968b641fda5d3b969',
      repo        : 'javyxu.github.io',
      owner       : 'javyxu',
      admin       : ['javyxu'],
      id          : '862818294a6947d59a582f33b766887e',
      proxy       : 'https://netnr-proxy.cloudno.de/https://github.com/login/oauth/access_token',
        language: 'en',
      distractionFreeMode: true
    });
    gitalk.render('gitalk-container');
  }, window.Gitalk);
});
</script>

</body>
</html>
